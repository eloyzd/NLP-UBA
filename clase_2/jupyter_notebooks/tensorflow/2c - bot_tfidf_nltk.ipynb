{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5hxxkdAQJg"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Bot con NLTK utilizando un corpus de wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kCED1hh-Ioyf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\zerba\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\zerba\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\zerba\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs\n",
        "\n",
        "import nltk\n",
        "# Descargar el diccionario\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOa4JPSCJ29"
      },
      "source": [
        "### Datos\n",
        "Se consumira los datos del artículo de wikipedia sobre el deporte \"tennis\" en ingles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIO7b8GjAC17"
      },
      "outputs": [],
      "source": [
        "# raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Harry_Potter')\n",
        "# raw_html = raw_html.read()\n",
        "# article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "# article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "# article_text = ''\n",
        "\n",
        "# for para in article_paragraphs:\n",
        "#     article_text += para.text\n",
        "\n",
        "# article_text = article_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_file = open(r\"C:\\Users\\zerba\\Downloads\\Harry Potter.txt\",encoding=\"utf8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_text = article_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUH30a1_rOkS"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "article_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "BtGLJjt6rQhK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de caracteres en la nota: 446153\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de caracteres en la nota:\", len(article_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHxBRNzCMOS"
      },
      "source": [
        "### 2 - Preprocesamiento\n",
        "- Remover caracteres especiales\n",
        "- Quitar espacios o saltos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "HnEUTD1Erl1N"
      },
      "outputs": [],
      "source": [
        "text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "text = re.sub(r'\\s+', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ycrAMYrn66"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PA5F0s4UsMpf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de caracteres en el texto: 56448\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de caracteres en el texto:\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKNcDGcisajf"
      },
      "source": [
        "### 3 - Dividir el texto en sentencias y en palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "reXBOFQ7sdlB"
      },
      "outputs": [],
      "source": [
        "corpus = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5GloV9fsi6o"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "hmQ7nkvvsi0i"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['They',\n",
              " 'were',\n",
              " 'the',\n",
              " 'last',\n",
              " 'people',\n",
              " 'you',\n",
              " '’',\n",
              " 'd',\n",
              " 'expect',\n",
              " 'to',\n",
              " 'be',\n",
              " 'involved',\n",
              " 'in',\n",
              " 'anything',\n",
              " 'strange',\n",
              " 'or',\n",
              " 'mysterious',\n",
              " ',',\n",
              " 'because',\n",
              " 'they']"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demos un vistazo\n",
        "words[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "YXPWNkKfEvDZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulario: 102004\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocabulario:\", len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlYKyb3OtDse"
      },
      "source": [
        "### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n",
        "- Lematizar los tokens de la oración\n",
        "- Quitar símbolos de puntuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "afPok8pstPOx"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def get_processed_text(document):\n",
        "    # 1 - reduce el texto a mínuscula\n",
        "    # 2 - quitar los simbolos de puntuacion\n",
        "    # 3 - realiza la tokenización\n",
        "    # 4 - realiza la lematización\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8r6d9ZuyR9"
      },
      "source": [
        "### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus de wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "IRYFHcBfk2Gt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_response(user_input, corpus):\n",
        "    response = ''\n",
        "    # Sumar al corpus la pregunta del usuario para calcular\n",
        "    # su cercania con otros documentos/sentencias\n",
        "    corpus.append(user_input)\n",
        "\n",
        "    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n",
        "    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "\n",
        "    # Crear los vectores a partir del corpus\n",
        "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n",
        "    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "\n",
        "    # Obtener el índice del vector más cercano a nuestra oración\n",
        "    # --> descartando la similitud contra nuestor vector propio\n",
        "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    if vector_matched == 0:\n",
        "        response = \"I am sorry, I could not understand you\"\n",
        "    else:\n",
        "        response = corpus[similar_sentence_number]\n",
        "    \n",
        "    corpus.remove(user_input)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-BuXPBybSp"
      },
      "source": [
        "### 6 - Ensayar el sistema\n",
        "El sistema intentará encontrar la parte del artículo que más se relaciona con nuestro texto de entrada. Sugerencias ensayar:\n",
        "- Grand slam\n",
        "- tournaments\n",
        "- nadal\n",
        "- artificial intelligence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Z2X4j8XyydSb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"c:\\Users\\zerba\\Documents\\UBA\\Repos\\Repositorios\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "# Se utilizará gradio para ensayar el bot\n",
        "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
        "# https://gradio.app/\n",
        "import sys\n",
        "!{sys.executable} -m pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "SZv5MiVzynG1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zerba\\Documents\\UBA\\Repos\\Repositorios de Eloy\\uba-env\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860/\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plant crying\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zerba\\Documents\\UBA\\Repos\\Repositorios de Eloy\\uba-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mandragora\n",
            "Mandragora\n",
            "Mandragora plant\n",
            "voldemort\n",
            "you know-who\n",
            "you-know-who\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x150b32a1f10>, 'http://127.0.0.1:7860/', None)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def bot_response(human_text):\n",
        "    print(human_text)\n",
        "    return generate_response(human_text.lower(), corpus)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=bot_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\",\n",
        "    layout=\"vertical\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2c - bot_tfidf_nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('uba-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef73d9334d5ce8cdc323e58bc2c037d76376afdd3fe87998db26b15bfda9212b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
